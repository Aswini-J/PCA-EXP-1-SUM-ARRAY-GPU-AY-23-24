{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "%load_ext nvcc_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "void checkResult(float *hostRef, float *gpuRef, const int N)\n",
    "{\n",
    "    double epsilon = 1.0E-8;\n",
    "    bool match = 1;\n",
    "\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        if (abs(hostRef[i] - gpuRef[i]) > epsilon)\n",
    "        {\n",
    "            match = 0;\n",
    "            printf(\"Arrays do not match!\\n\");\n",
    "            printf(\"host %5.2f gpu %5.2f at current %d\\n\", hostRef[i],\n",
    "                   gpuRef[i], i);\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (match) printf(\"Arrays match.\\n\\n\");\n",
    "\n",
    "    return;\n",
    "}\n",
    "\n",
    "void initialData(float *ip, int size)\n",
    "{\n",
    "    // generate different seed for random number\n",
    "    time_t t;\n",
    "    srand((unsigned) time(&t));\n",
    "\n",
    "    for (int i = 0; i < size; i++)\n",
    "    {\n",
    "        ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n",
    "    }\n",
    "\n",
    "    return;\n",
    "}\n",
    "\n",
    "void sumArraysOnHost(float *A, float *B, float *C, const int N)\n",
    "{\n",
    "    for (int idx = 0; idx < N; idx++)\n",
    "    {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Refer sumArraysOnGPU-timer.cu and write your kernel function to perform parallel addition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "    printf(\"%s Starting...\\n\", argv[0]);\n",
    "\n",
    "    // set up device\n",
    "    int dev = 0;\n",
    "    cudaDeviceProp deviceProp;\n",
    "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
    "    printf(\"Using Device %d: %s\\n\", dev, deviceProp.name);\n",
    "    CHECK(cudaSetDevice(dev));\n",
    "\n",
    "    // set up data size of vectors\n",
    "    int nElem = 1 << 24;\n",
    "    printf(\"Vector size %d\\n\", nElem);\n",
    "\n",
    "    // malloc host memory\n",
    "    size_t nBytes = nElem * sizeof(float);\n",
    "\n",
    "    float *h_A, *h_B, *hostRef, *gpuRef;\n",
    "    h_A     = (float *)malloc(nBytes);\n",
    "    h_B     = (float *)malloc(nBytes);\n",
    "    hostRef = (float *)malloc(nBytes);\n",
    "    gpuRef  = (float *)malloc(nBytes);\n",
    "\n",
    "    double iStart, iElaps;\n",
    "\n",
    "    // initialize data at host side\n",
    "    iStart = seconds();\n",
    "    initialData(h_A, nElem);\n",
    "    initialData(h_B, nElem);\n",
    "    iElaps = seconds() - iStart;\n",
    "    printf(\"initialData Time elapsed %f sec\\n\", iElaps);\n",
    "    memset(hostRef, 0, nBytes);\n",
    "    memset(gpuRef,  0, nBytes);\n",
    "\n",
    "    // add vector at host side for result checks\n",
    "    iStart = seconds();\n",
    "    sumArraysOnHost(h_A, h_B, hostRef, nElem);\n",
    "    iElaps = seconds() - iStart;\n",
    "    printf(\"sumArraysOnHost Time elapsed %f sec\\n\", iElaps);\n",
    "\n",
    "    // malloc device global memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    CHECK(cudaMalloc((float**)&d_A, nBytes));\n",
    "    CHECK(cudaMalloc((float**)&d_B, nBytes));\n",
    "    CHECK(cudaMalloc((float**)&d_C, nBytes));\n",
    "\n",
    "    // transfer data from host to device\n",
    "    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n",
    "    CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n",
    "    CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));\n",
    "\n",
    "    // invoke kernel at host side\n",
    "    int iLen = 512;\n",
    "    dim3 block (iLen);\n",
    "    dim3 grid  ((nElem + block.x - 1) / block.x);\n",
    "\n",
    "    iStart = seconds();\n",
    "    sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);\n",
    "    CHECK(cudaDeviceSynchronize());\n",
    "    iElaps = seconds() - iStart;\n",
    "    printf(\"sumArraysOnGPU <<<  %d, %d  >>>  Time elapsed %f sec\\n\", grid.x,\n",
    "           block.x, iElaps);\n",
    "\n",
    "    // check kernel error\n",
    "    CHECK(cudaGetLastError()) ;\n",
    "\n",
    "    // copy kernel result back to host side\n",
    "    CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n",
    "\n",
    "    // check device results\n",
    "    checkResult(hostRef, gpuRef, nElem);\n",
    "\n",
    "    // free device global memory\n",
    "    CHECK(cudaFree(d_A));\n",
    "    CHECK(cudaFree(d_B));\n",
    "    CHECK(cudaFree(d_C));\n",
    "\n",
    "    // free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(hostRef);\n",
    "    free(gpuRef);\n",
    "\n",
    "    return(0);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above code to change the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Elapsed time Differences"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
